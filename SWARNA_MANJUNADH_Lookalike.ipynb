{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj7Xn-PTK5fl",
        "outputId": "267d4206-b544-41bb-8f0c-8aff36c39ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas numpy scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Load datasets\n",
        "customers = pd.read_csv('Customers.csv')\n",
        "products = pd.read_csv('Products.csv')\n",
        "\n",
        "# Assume Transactions.csv exists or create a sample DataFrame if needed\n",
        "transactions = pd.DataFrame({\n",
        "    'CustomerID': ['C0001', 'C0001', 'C0002', 'C0003', 'C0003'],\n",
        "    'ProductID': ['P001', 'P002', 'P003', 'P001', 'P003'],\n",
        "    'PurchaseAmount': [50, 20, 100, 70, 80],\n",
        "    'Category': ['Electronics', 'Books', 'Clothing', 'Electronics', 'Clothing']\n",
        "})\n"
      ],
      "metadata": {
        "id": "yqMlr2u_NQdq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customers['SignupDate'] = pd.to_datetime(customers['SignupDate'])\n",
        "customers['DaysSinceSignup'] = (pd.Timestamp.now() - customers['SignupDate']).dt.days\n"
      ],
      "metadata": {
        "id": "loEGNRmOO8tt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transaction_summary = transactions.groupby('CustomerID').agg(\n",
        "    TotalSpend=('PurchaseAmount', 'sum'),\n",
        "    AvgSpend=('PurchaseAmount', 'mean'),\n",
        "    MostFreqCategory=('Category', lambda x: x.mode()[0])  # Most common category\n",
        ").reset_index()\n",
        "\n",
        "# Merge with customers data\n",
        "data = pd.merge(customers, transaction_summary, on='CustomerID', how='left')\n"
      ],
      "metadata": {
        "id": "4vSfuytOPBXe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('region', OneHotEncoder(), ['Region']),\n",
        "        ('signup_days', 'passthrough', ['DaysSinceSignup']),\n",
        "        ('transaction_features', 'passthrough', ['TotalSpend', 'AvgSpend'])\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Transform features\n",
        "customer_features = preprocessor.fit_transform(data)\n",
        "\n",
        "# Normalize for similarity calculations\n",
        "scaler = StandardScaler()\n",
        "customer_features = scaler.fit_transform(customer_features)\n"
      ],
      "metadata": {
        "id": "4FnUGtSuPFD_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# ... (Your existing code for loading and preprocessing data) ...\n",
        "\n",
        "# Impute missing values (replace NaNs) before feature scaling\n",
        "imputer = SimpleImputer(strategy='mean') # or 'median', 'most_frequent', 'constant'\n",
        "data[['TotalSpend', 'AvgSpend']] = imputer.fit_transform(data[['TotalSpend', 'AvgSpend']])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('region', OneHotEncoder(), ['Region']),\n",
        "        ('signup_days', 'passthrough', ['DaysSinceSignup']),\n",
        "        ('transaction_features', 'passthrough', ['TotalSpend', 'AvgSpend'])\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Transform features\n",
        "customer_features = preprocessor.fit_transform(data)\n",
        "\n",
        "# Normalize for similarity calculations\n",
        "scaler = StandardScaler()\n",
        "customer_features = scaler.fit_transform(customer_features)\n",
        "\n",
        "# Compute similarity matrix\n",
        "similarity_matrix = cosine_similarity(customer_features)"
      ],
      "metadata": {
        "id": "QCFhFzIMPIuG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate recommendations\n",
        "recommendations = {}\n",
        "for i in range(20):  # First 20 customers\n",
        "    similar_indices = np.argsort(-similarity_matrix[i])[1:4]  # Exclude self\n",
        "    recommendations[data['CustomerID'].iloc[i]] = [\n",
        "        (data['CustomerID'].iloc[j], similarity_matrix[i][j])\n",
        "        for j in similar_indices\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "Yy63Pk6NPe8K"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save recommendations to Lookalike.csv\n",
        "output_df = pd.DataFrame({\n",
        "    'CustomerID': recommendations.keys(),\n",
        "    'Recommendations': recommendations.values()\n",
        "})\n",
        "output_df.to_csv('Lookalike.csv', index=False)\n"
      ],
      "metadata": {
        "id": "5XzKtyBsPnHR"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load datasets\n",
        "customers = pd.read_csv('Customers.csv')\n",
        "transactions = pd.read_csv('Transactions.csv')\n",
        "products = pd.read_csv('Products.csv')\n",
        "\n",
        "# Step 1: Merge Transactions with Product Data\n",
        "transactions = pd.merge(transactions, products[['ProductID', 'Category', 'Price']], on='ProductID', how='left')\n",
        "\n",
        "# Step 2: Aggregate Transaction Data by CustomerID\n",
        "transaction_summary = transactions.groupby('CustomerID').agg(\n",
        "    TotalSpend=('TotalValue', 'sum'),\n",
        "    AvgSpend=('TotalValue', 'mean'),\n",
        "    TransactionCount=('TransactionID', 'count'),\n",
        "    MostFreqCategory=('Category', lambda x: x.mode()[0])  # Most frequent product category\n",
        ").reset_index()\n",
        "\n",
        "# Step 3: Merge Transaction Summary with Customer Data\n",
        "data = pd.merge(customers, transaction_summary, on='CustomerID', how='left')\n",
        "\n",
        "# Feature: Days since Signup\n",
        "data['SignupDate'] = pd.to_datetime(data['SignupDate'])\n",
        "data['DaysSinceSignup'] = (pd.Timestamp.now() - data['SignupDate']).dt.days\n",
        "\n",
        "# Step 4: Data Preprocessing (Handling missing data and scaling)\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('region', OneHotEncoder(), ['Region']),\n",
        "        ('signup_days', 'passthrough', ['DaysSinceSignup']),\n",
        "        ('transaction_features', 'passthrough', ['TotalSpend', 'AvgSpend', 'TransactionCount'])\n",
        "    ]\n",
        ")\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "data[['TotalSpend', 'AvgSpend', 'TransactionCount']] = imputer.fit_transform(data[['TotalSpend', 'AvgSpend', 'TransactionCount']])\n",
        "\n",
        "# Transform the features\n",
        "customer_features = preprocessor.fit_transform(data)\n",
        "\n",
        "# Normalize for similarity calculations\n",
        "scaler = StandardScaler()\n",
        "customer_features = scaler.fit_transform(customer_features)\n",
        "\n",
        "# Step 5: Compute Similarity Matrix\n",
        "similarity_matrix = cosine_similarity(customer_features)\n",
        "\n",
        "# Step 6: Generate Recommendations for the first 20 customers (C0001 - C0020)\n",
        "recommendations = {}\n",
        "for i in range(20):  # First 20 customers\n",
        "    similar_indices = np.argsort(-similarity_matrix[i])[1:4]  # Exclude self (similarity[i][i] = 1)\n",
        "    recommendations[data['CustomerID'].iloc[i]] = [\n",
        "        (data['CustomerID'].iloc[j], similarity_matrix[i][j])\n",
        "        for j in similar_indices\n",
        "    ]\n",
        "\n",
        "# Step 7: Prepare the Output Data for Lookalike.csv\n",
        "output_data = []\n",
        "\n",
        "for cust_id, similar_customers in recommendations.items():\n",
        "    similar_str = '; '.join([f'{cust}: {score:.4f}' for cust, score in similar_customers])\n",
        "    output_data.append({'CustomerID': cust_id, 'Recommendations': similar_str})\n",
        "\n",
        "# Save to Lookalike.csv\n",
        "output_df = pd.DataFrame(output_data)\n",
        "output_df.to_csv('Lookalike.csv', index=False)\n",
        "\n",
        "print(\"Lookalike recommendations saved to 'Lookalike.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R22tUp1jWs36",
        "outputId": "a45aba54-de10-403a-d403-86fbf012d613"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lookalike recommendations saved to 'Lookalike.csv'.\n"
          ]
        }
      ]
    }
  ]
}